{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Modèle Random Forest\n",
    "Ce modèle a été moins développé que les autres mais le code vous est accessible\n",
    "## Initialisation de PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/05 22:11:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/05 22:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/01/05 22:11:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext created\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import HashingTF, CountVectorizer, Tokenizer, IDF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "spark = SparkSession.builder.master('local[16]').getOrCreate()\n",
    "\n",
    "print(\"SparkContext created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Récupération des données et répartitions des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class repartitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:===================================>                     (10 + 6) / 16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| class| count|\n",
      "+------+------+\n",
      "|target|    16|\n",
      "|     4|800000|\n",
      "|     0|800000|\n",
      "+------+------+\n",
      "\n",
      "Dataset schema :\n",
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "filename = 'hdfs://localhost:9000/sentiment/clean/'\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n",
    "\n",
    "df = spark.read.options(inferSchema=True,\n",
    "                        ignoreLeadingWhiteSpace=True,\n",
    "                        schema=schema).csv(filename)\n",
    "df = df.dropna()\n",
    "\n",
    "official_col = ['class', 'tweet_id', 'date', 'query', 'username', 'content']\n",
    "\n",
    "for i, column in enumerate(df.columns):\n",
    "    df = df.withColumnRenamed(column, official_col[i])\n",
    "\n",
    "df = df.select(\"class\",\"content\")\n",
    "\n",
    "print(\"Dataset class repartitions\")\n",
    "gr = df.groupBy(\"class\").count()\n",
    "gr.show()\n",
    "\n",
    "print(\"Dataset schema :\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Traitement des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# chooses CountVectorize or HashingTF\n",
    "cVec = False\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "df = tokenizer.transform(df)\n",
    "\n",
    "if cVec:\n",
    "    cv = CountVectorizer(inputCol=\"words\", outputCol=\"r_features\")\n",
    "    df = cv.fit(df)\n",
    "else:\n",
    "    hashtf = HashingTF(inputCol=\"words\", outputCol=\"r_features\")\n",
    "    df = hashtf.transform(df)\n",
    "\n",
    "idf = IDF(inputCol=\"r_features\", outputCol=\"features\")\n",
    "\n",
    "step = idf.fit(df)\n",
    "df = step.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Partage entrainement et test (80/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# split the data into training and test sets\n",
    "train, test = df.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Entrainement et évaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"class\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf = RandomForestClassifier(labelCol=\"class\", featuresCol=\"features\", maxDepth=10)\n",
    "\n",
    "model = rf.fit(train)\n",
    "\n",
    "# run on test data\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# evaluate\n",
    "print()\n",
    "print(\"Accuracy = \", evaluator.evaluate(predictions))\n",
    "\n",
    "predictions.groupBy('class','prediction').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "69132f190f4b668d460737ebcde5689e94028e0059afe55fa12d464dbc238f4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
